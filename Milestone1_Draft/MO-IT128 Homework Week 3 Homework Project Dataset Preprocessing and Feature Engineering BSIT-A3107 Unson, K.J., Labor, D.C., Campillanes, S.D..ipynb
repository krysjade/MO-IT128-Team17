{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2eb6d16-3acf-43f7-ab64-837640d3aeb6",
   "metadata": {},
   "source": [
    "# Data Preprocessing Decisions\n",
    "- Transaction dates were converted to a standard datetime format to enable time-based analysis and feature creation.\n",
    "- Missing numerical values in transaction amounts and customer satisfaction scores were handled using median imputation to minimize the influence of extreme values and preserve overall data distribution.\n",
    "- Extreme transaction amounts were capped at the 99th percentile to reduce the impact of outliers on aggregated customer spending metrics.\n",
    "- Customer satisfaction scores were clipped to a maximum value of 10 to ensure consistency with the expected rating scale.\n",
    "- Duplicate records were removed from all datasets to maintain data integrity, using unique identifiers such as Transaction_ID and Product_ID.\n",
    "- Non-essential or redundant fields (e.g., target age group in the product dataset) were removed to simplify the dataset and focus on analytically relevant attributes.\n",
    "- Datasets were aggregated and merged at the customer level to support customer-centric analysis aligned with FinMark’s business objectives.\n",
    "\n",
    "# Feature Engineering Decisions\n",
    "- Transaction data was aggregated per customer to generate key behavioral metrics, including total spend, average transaction value, transaction frequency, and number of unique transaction types.\n",
    "- A recency feature was created by calculating the number of days since a customer’s most recent transaction, enabling analysis of customer engagement over time.\n",
    "- The most frequent transaction type per customer was identified to capture primary customer behavior patterns.\n",
    "- Customer feedback data was aggregated to compute average satisfaction and likelihood-to-recommend scores.\n",
    "- For customers without feedback records, missing feedback metrics were filled using median values to retain these customers in downstream analysis.\n",
    "- A High Spender indicator was created based on the top 25% of total customer spending to support segmentation and targeting use cases.\n",
    "- A Loyalty Index was engineered by combining customer satisfaction and transaction frequency, reflecting both engagement and sentiment in a single metric.\n",
    "\n",
    "# Data Export\n",
    "- Cleaned datasets and the engineered customer feature dataset were exported as CSV files to support reproducibility and downstream modeling or reporting tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e8ab8eb-9fec-477e-8464-5fa4104dec2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e0472ff-7f79-4ea5-bb42-0790dd3238ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "transactions = pd.read_csv('Transaction_Data.csv')\n",
    "products = pd.read_csv('Product_Offering_Data.csv')\n",
    "feedback = pd.read_csv('Customer_Feedback_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c39d0ad-d491-409a-b452-6aa6d53223c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MERGING BEFORE CLEANING ---\n",
    "\n",
    "# Merge transaction data with customer feedback at the transaction level\n",
    "tx_cust = pd.merge(\n",
    "    transactions,\n",
    "    feedback,\n",
    "    on='Customer_ID',\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db781b05-d065-42f2-be26-9db466f1f7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PREPROCESSING ON MERGED DATA ---\n",
    "\n",
    "# 1. Convert transaction date to datetime\n",
    "tx_cust['Transaction_Date'] = pd.to_datetime(tx_cust['Transaction_Date'])\n",
    "\n",
    "# 2. Handle missing transaction amounts using median of the merged data\n",
    "tx_cust['Transaction_Amount'] = tx_cust['Transaction_Amount'].fillna(\n",
    "    tx_cust['Transaction_Amount'].median()\n",
    ")\n",
    "\n",
    "# 3. Cap extreme transaction amounts at the 99th percentile\n",
    "cap_val = tx_cust['Transaction_Amount'].quantile(0.99)\n",
    "tx_cust['Transaction_Amount'] = tx_cust['Transaction_Amount'].clip(upper=cap_val)\n",
    "\n",
    "# 4. Handle satisfaction score:\n",
    "#    - Impute missing values with median\n",
    "#    - Clip to max 10\n",
    "if 'Satisfaction_Score' in tx_cust.columns:\n",
    "    tx_cust['Satisfaction_Score'] = tx_cust['Satisfaction_Score'].fillna(\n",
    "        tx_cust['Satisfaction_Score'].median()\n",
    "    )\n",
    "    tx_cust['Satisfaction_Score'] = tx_cust['Satisfaction_Score'].clip(upper=10.0)\n",
    "\n",
    "# 5. Remove duplicate transaction records (using Transaction_ID)\n",
    "tx_cust = tx_cust.drop_duplicates(subset=['Transaction_ID'])\n",
    "\n",
    "# 6. Clean Product Offering Data separately (since it is not directly in tx_cust)\n",
    "products = products.drop_duplicates(subset=['Product_ID'])\n",
    "if 'Target_Age_Group' in products.columns:\n",
    "    products = products.drop(columns=['Target_Age_Group'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1197d00-cf00-47da-8462-45fdaa730a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- FEATURE ENGINEERING (AGGREGATE PER CUSTOMER) ---\n",
    "\n",
    "# Latest transaction date for recency computation\n",
    "latest_date = tx_cust['Transaction_Date'].max()\n",
    "\n",
    "# Aggregate transaction behavior per customer\n",
    "cust_agg = tx_cust.groupby('Customer_ID').agg(\n",
    "    Total_Spend=('Transaction_Amount', 'sum'),\n",
    "    Avg_Transaction_Value=('Transaction_Amount', 'mean'),\n",
    "    Transaction_Count=('Transaction_ID', 'count'),\n",
    "    Last_Transaction_Date=('Transaction_Date', 'max'),\n",
    "    Unique_Transaction_Types=('Transaction_Type', 'nunique')\n",
    ").reset_index()\n",
    "\n",
    "# Recency in days since last transaction\n",
    "cust_agg['Recency'] = (latest_date - cust_agg['Last_Transaction_Date']).dt.days\n",
    "\n",
    "# Most frequent transaction type per customer\n",
    "mode_type = tx_cust.groupby('Customer_ID')['Transaction_Type'] \\\n",
    "                   .agg(lambda x: x.mode()[0]) \\\n",
    "                   .reset_index()\n",
    "mode_type.rename(columns={'Transaction_Type': 'Primary_Transaction_Type'}, inplace=True)\n",
    "\n",
    "# Aggregate feedback data at customer level\n",
    "# (using the cleaned Satisfaction_Score from tx_cust)\n",
    "feed_agg = tx_cust.groupby('Customer_ID').agg(\n",
    "    Avg_Satisfaction_Score=('Satisfaction_Score', 'mean'),\n",
    "    Avg_Likelihood_to_Recommend=('Likelihood_to_Recommend', 'mean')\n",
    ").reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc7559e2-f91f-4211-93e8-0433273b71d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MERGE INTO MASTER DATASET ---\n",
    "\n",
    "master_df = pd.merge(cust_agg, mode_type, on='Customer_ID', how='left')\n",
    "master_df = pd.merge(master_df, feed_agg, on='Customer_ID', how='left')\n",
    "\n",
    "# Fill missing feedback metrics (customers with no feedback) using medians\n",
    "master_df['Avg_Satisfaction_Score'] = master_df['Avg_Satisfaction_Score'].fillna(\n",
    "    master_df['Avg_Satisfaction_Score'].median()\n",
    ")\n",
    "master_df['Avg_Likelihood_to_Recommend'] = master_df['Avg_Likelihood_to_Recommend'].fillna(\n",
    "    master_df['Avg_Likelihood_to_Recommend'].median()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42d13caa-50f0-43cc-99e0-c1189ba08159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- NEW METRICS: HIGH SPENDER, LOYALTY INDEX, INCOME LEVEL GROUP ---\n",
    "\n",
    "# High Spender indicator (top 25% of total spend)\n",
    "master_df['Is_High_Spender'] = (\n",
    "    master_df['Total_Spend'] > master_df['Total_Spend'].quantile(0.75)\n",
    ").astype(int)\n",
    "\n",
    "# Loyalty Index: satisfaction × transaction frequency\n",
    "master_df['Loyalty_Index'] = (\n",
    "    master_df['Avg_Satisfaction_Score'] * master_df['Transaction_Count']\n",
    ")\n",
    "\n",
    "# Income Level Group based on Total_Spend using relative positions (terciles)\n",
    "# 0–33%  -> Low\n",
    "# 33–66% -> Medium\n",
    "# 66–100% -> High\n",
    "q1 = master_df['Total_Spend'].quantile(1/3)\n",
    "q2 = master_df['Total_Spend'].quantile(2/3)\n",
    "\n",
    "def income_group(amount):\n",
    "    if amount <= q1:\n",
    "        return 'Low'\n",
    "    elif amount <= q2:\n",
    "        return 'Medium'\n",
    "    else:\n",
    "        return 'High'\n",
    "\n",
    "master_df['Income_Level_Group'] = master_df['Total_Spend'].apply(income_group)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f1c0a07-bedf-417b-b9b4-a71ddf24b35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- EXPORT CLEANED AND ENGINEERED DATASETS ---\n",
    "\n",
    "# Transaction-level merged and cleaned data\n",
    "tx_cust.to_csv('Cleaned_Transactions_Merged.csv', index=False)\n",
    "\n",
    "# Cleaned products\n",
    "products.to_csv('Cleaned_Products.csv', index=False)\n",
    "\n",
    "# Customer-level engineered features (with income level group)\n",
    "master_df.to_csv('Engineered_Customer_Features.csv', index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
